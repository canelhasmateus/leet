{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import database\n",
    "import pandas as pd\n",
    "\n",
    "voos = pd.read_sql( '''\n",
    "select * from voos_sp_sj\n",
    "where dt_partida_prevista >= '2018-12-30 00:00:00'\n",
    "order by dt_partida_prevista''', database.engine )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "clima = pd.read_sql( \"\"\"\n",
    "\n",
    "with\n",
    "\tmappings as (\n",
    "\tselect *\n",
    "\t\tfrom\n",
    "\t\t\taero_ponto_coleta\n",
    "\t\twhere\n",
    "\t\t\texists(\n",
    "\t\t\t\tselect\n",
    "\t\t\t\t\t1\n",
    "\t\t\t\t\tfrom\n",
    "\t\t\t\t\t\tvoos_sp_sj voos\n",
    "\t\t\t\t\twhere\n",
    "\t\t\t\t\t\tvoos.id_aerodromo_origem = aero_ponto_coleta.id_aerodromo\n",
    "\t\t\t\t)\n",
    "\n",
    "\n",
    "\t)\n",
    "  , resumo_clima as (\n",
    "                    select\n",
    "\t                    ponto_id\n",
    "\t                  , date( dt_clima ) - interval 1 day as dt_clima\n",
    "\t                  , id_aerodromo\n",
    "\t                  , sum( precipitacao_total )         as precipitacao_total\n",
    "\t                  , avg( temperatura_ar )             as temperatura_ar\n",
    "\t                  , avg( vento_rajada_maxima )        as vento_rajada_maxima\n",
    "\t                    from\n",
    "\t\t                    clima\n",
    "\t\t                    inner join mappings using ( ponto_id )\n",
    "\t                    group by\n",
    "\t\t                    ponto_id\n",
    "\t\t                  , date( dt_clima )\n",
    "                    )\n",
    "\n",
    "select * from resumo_clima\n",
    "                    \"\"\", database.engine )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "voos[ \"atrasos_partida\" ] = (voos[ \"dt_partida_prevista\" ] - voos[ \"dt_partida_real\" ]).dt.seconds / 60\n",
    "voos[ \"atrasos_chegada\" ] = (voos[ \"dt_chegada_prevista\" ] - voos[ \"dt_chegada_real\" ]).dt.seconds / 60\n",
    "# Estaremos preocupados apenas em prever os atrasos de partida.\n",
    "# Um caso estranho que vemos aqui é que tanto atrasos_chegada e atrasos_partida tem um valor de minimo == 0.\n",
    "# Não seria de se esperar que houvessem chegadas e partidas adiantadas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "voos[ \"atrasado\" ] = voos[ \"atrasos_partida\" ] >= 20\n",
    "voos[ \"atrasado_previsto\" ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Um ponto interessante de se considerar é a metrica utilizada para\n",
    "# 'scorar' o modelo.\n",
    "# Acuracia vs F1 vs Precisao são algumas possibilidades,\n",
    "# E todas tem uma interpretação levemente diferente.\n",
    "\n",
    "# Se pensarmos nas 4 possibilidades, vulgo:\n",
    "# Dizemos que atrasa, mas ele não atrasa ( Falso positivo )\n",
    "# Dizemos que atrasa, e de fato atrasa ( Verdadeiro Positivo)\n",
    "# não Dizemos que atrasa, e ele atrasa ( Falso negativo)\n",
    "# não Dizemos que atrasa, e ele não atrasa (Verdadeiro Negativo).\n",
    "\n",
    "# Vamos pensar na idéia de enviar um ( push | email ) para o cliente, caso acharmos que o voo dele irá atrasar.\n",
    "# Nesse caso,\n",
    "# Falsos positivos são PÉSSIMOS. ( 1000x )\n",
    "# Verdadeiros positivos são Bons. ( 100x )\n",
    "# Falsos negativos é regular ( 10x )\n",
    "# Verdadeiro positivo é regular ( 1x )\n",
    "from sklearn.metrics import accuracy_score, precision_score, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_columns = [\n",
    "\t\t\"nr_voo\",\n",
    "\t\t\"sg_di\",\n",
    "\t\t# \"id_tipo_linha\",\n",
    "\t\t# \"dt_partida_prevista\",\n",
    "\t\t\"id_empresa\",\n",
    "\t\t\"id_aerodromo_origem\",\n",
    "\t\t\"id_aerodromo_destino\",\n",
    "\t\t]\n",
    "df_y = voos[ [ \"atrasado\" ] ]\n",
    "df_x = voos[ train_columns ]\n",
    "\n",
    "df_train_x, df_test_x, df_train_y, df_test_y = train_test_split( df_x, df_y, train_size = 0.8 )\n",
    "\n",
    "print( \"training accuracy_score: \", accuracy_score( df_train_y, [ 0 ] * df_train_y.shape[ 0 ] ) )\n",
    "print( \"training precision_score: \", precision_score( df_train_y, [ 0 ] * df_train_y.shape[ 0 ] ) )\n",
    "print( \"training f1_score: \", f1_score( df_train_y, [ 0 ] * df_train_y.shape[ 0 ] ) )\n",
    "print( \"training roc_auc_score: \", roc_auc_score( df_train_y, [ 0 ] * df_train_y.shape[ 0 ] ) )\n",
    "\n",
    "print( \"testing accuracy_score: \", accuracy_score( df_test_y, [ 0 ] * df_test_y.shape[ 0 ] ) )\n",
    "print( \"testing precision_score: \", precision_score( df_test_y, [ 0 ] * df_test_y.shape[ 0 ] ) )\n",
    "print( \"testing f1_score: \", f1_score( df_test_y, [ 0 ] * df_test_y.shape[ 0 ] ) )\n",
    "print( \"testing roc_auc_score: \", roc_auc_score( df_test_y, [ 0 ] * df_test_y.shape[ 0 ] ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# O que vem a ser id_justificativa?\n",
    "\n",
    "# Usar as os pesos definidos como metricas\n",
    "# Considerar precisão porque nossa principal preocupação são falsos positivos.\n",
    "\n",
    "\n",
    "encoder = TargetEncoder(\n",
    "\t\tcols = train_columns,\n",
    "\t\thandle_unknown = 'value' ).fit( df_train_x, df_train_y )\n",
    "\n",
    "df_train_x = encoder.transform( df_train_x )\n",
    "df_test_x = encoder.transform( df_test_x )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.testing import all_estimators\n",
    "from sklearn.base import ClassifierMixin\n",
    "\n",
    "classifiers = [ est for est in all_estimators() if issubclass( est[ 1 ], ClassifierMixin ) ]\n",
    "print( classifiers )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from lazypredict.Supervised import LazyClassifier, CLASSIFIERS\n",
    "\n",
    "classifier = LazyClassifier( verbose = 1, predictions = True, ignore_warnings = False, classifiers = [ LogisticRegression, RandomForestClassifier, DecisionTreeClassifier,\n",
    "                                                                                                       SGDClassifier ] )\n",
    "classifier, predictions = classifier.fit( df_train_x, df_test_x, df_train_y, df_test_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print( \"training accuracy_score: \", accuracy_score( df_train_y, df_predict_train_y ) )\n",
    "print( \"training precision_score: \", precision_score( df_train_y, df_predict_train_y ) )\n",
    "print( \"training f1_score: \", f1_score( df_train_y, df_predict_train_y ) )\n",
    "print( \"training roc_auc_score: \", roc_auc_score( df_train_y, df_predict_train_y ) )\n",
    "print()\n",
    "print( \"testing accuracy_score: \", accuracy_score( df_test_y, df_predict_test_y ) )\n",
    "print( \"testing precision_score: \", precision_score( df_test_y, df_predict_test_y ) )\n",
    "print( \"testing f1_score: \", f1_score( df_test_y, df_predict_test_y ) )\n",
    "print( \"testing roc_auc_score: \", roc_auc_score( df_test_y, df_predict_test_y ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
